{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e07f70-9b15-4153-866a-bc788c2ad395",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://universe.roboflow.com/test1-2vmvi/test1-9vnrj/browse?queryText=&pageSize=50&startingIndex=0&browseQuery=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6221690-f2c1-45e6-aec7-867a0917dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://universe.roboflow.com/insulator-defect-detection-cyjg2/smear-detection/dataset/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d335496-b924-4036-839c-688513f64225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d5c80ab-6125-4ad2-b9cc-fd81fc606491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212a83bd-a2b3-4211-a0e9-7182e92647a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "633a0d5e-2678-4c78-8532-0c5093d42d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac56f6e6ea79ce6e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# IMAGES_DIR = 'cv_open_dataset/open_img/'  # Путь к вашему датасету с изображениями\n",
    "# MASKS_DIR = 'cv_open_dataset/open_msk/'  # Путь к вашему датасету с масками\n",
    "# IMAGES_DIR = 'cv_synt_dataset/synt_img/'  # Путь к вашему датасету с изображениями\n",
    "# MASKS_DIR = 'cv_synt_dataset/synt_msk/'  # Путь к вашему датасету с масками\n",
    "IMAGES_DIR = 'cv_new_dataset_test/rgbImages/'  # Путь к вашему датасету с изображениями\n",
    "MASKS_DIR = 'cv_new_dataset_test/rgbLabels/'  # Путь к вашему датасету с масками\n",
    "OUTPUT_DIR = './datasets/train_data'  # Путь к выходной директории\n",
    "TRAIN_SIZE = 0.8  # Процент обучающей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "406044194f9535d5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(OUTPUT_DIR, 'images/train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'images/val'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'labels/train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'labels/val'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61ba29c30ff4a2dc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "image_files = [f for f in os.listdir(IMAGES_DIR) if f.endswith(('.jpg', '.png'))]\n",
    "mask_files = [f for f in os.listdir(MASKS_DIR) if f.endswith('.png')]\n",
    "if len(image_files) != len(mask_files):\n",
    "    print(len(image_files))\n",
    "    print(len(mask_files))\n",
    "    print(\"Количество изображений и масок не совпадает.\")\n",
    "    for mask in mask_files:\n",
    "        if mask.replace(\".png\", \".jpg\") not in image_files:\n",
    "            print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d00ab007653eda31",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_images, val_images = train_test_split(image_files, train_size=TRAIN_SIZE, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4cd15221eb3f864",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def convert_mask_to_yolo(mask_path, out):\n",
    "    # Открываем изображение\n",
    "    image = cv2.imread(mask_path)\n",
    "\n",
    "    if image is None:\n",
    "        print(f\"Не удалось открыть изображение: {mask_path}\")\n",
    "        return\n",
    "    \n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Создаем маску для черного цвета\n",
    "    black_mask = cv2.inRange(image, (0, 0, 0), (50, 50, 50))\n",
    "\n",
    "    # Создаем новое изображение, где черный цвет остается, а остальные цвета становятся белыми\n",
    "    new_image = np.ones_like(image) * 255  # Начинаем с белого изображения\n",
    "    new_image[black_mask > 0] = [0, 0, 0]  # Заменяем черные пиксели\n",
    "\n",
    "    # Преобразуем в градации серого для нахождения контуров\n",
    "    gray_image = cv2.cvtColor(new_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Находим контуры\n",
    "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Сохраняем контуры в текстовом формате\n",
    "    output_file_name = os.path.splitext(os.path.basename(mask_path))[0] + '.txt'\n",
    "    output_file_path = os.path.join(OUTPUT_DIR, out, output_file_name)\n",
    "\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        for index, contour in enumerate(contours):\n",
    "            # Получаем координаты всех точек контура\n",
    "            contour_points = contour.reshape(-1, 2)\n",
    "            # Нормализуем координаты\n",
    "            normalized_points = [(x / width, y / height) for x, y in contour_points]\n",
    "            points_str = ' '.join(f\"{x:.3f} {y:.3f}\" for x, y in normalized_points)\n",
    "            f.write(f\"0 {points_str}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fdf99ded756441b8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Копирование изображений и масок в соответствующие папки\n",
    "for img in train_images:\n",
    "    shutil.copy(os.path.join(IMAGES_DIR, img), os.path.join(OUTPUT_DIR, 'images/train', img))\n",
    "    mask_name = img.replace('.jpg', '.png')\n",
    "    convert_mask_to_yolo(os.path.join(MASKS_DIR, mask_name), 'labels/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b2fd246643ce5cf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for img in val_images:\n",
    "    shutil.copy(os.path.join(IMAGES_DIR, img), os.path.join(OUTPUT_DIR, 'images/val', img))\n",
    "    mask_name = img.replace('.jpg', '.png')\n",
    "    convert_mask_to_yolo(os.path.join(MASKS_DIR, mask_name), 'labels/val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d7767e7853d7b98",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Датасет успешно разбит и сохранен в структуре проекта.\n"
     ]
    }
   ],
   "source": [
    "data_yaml_content = f\"\"\"\n",
    "train: train_data/images/train\n",
    "val: train_data/images/val\n",
    "\n",
    "nc: 1  # Обновите количество классов (1 для загрязнения)\n",
    "names: ['contaminated']  # Обновите названия классов\n",
    "\"\"\"\n",
    "\n",
    "with open('data.yaml', 'w') as f:\n",
    "    f.write(data_yaml_content)\n",
    "\n",
    "print(\"Датасет успешно разбит и сохранен в структуре проекта.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a73b014c45fcc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"baseline6.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17f1d200-a95b-4c97-b43a-c4efca4619d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d85abfbc10ae302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.47 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.44 🚀 Python-3.8.19 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce GTX 1660 SUPER, 5929MiB)\n",
      "WARNING ⚠️ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=segment, mode=train, model=baseline6.pt, data=./data.yaml, epochs=100, time=None, patience=4, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=None, name=train5, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.0003, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/segment/train5\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    683635  ultralytics.nn.modules.head.Segment          [1, 32, 64, [64, 128, 256]]   \n",
      "YOLO11n-seg summary: 355 layers, 2,842,803 parameters, 2,842,787 gradients, 10.4 GFLOPs\n",
      "\n",
      "Transferred 111/561 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks failed ❌. AMP training on NVIDIA GeForce GTX 1660 SUPER GPU may cause NaN losses or zero-mAP results, so AMP will be disabled during training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/dm/Документы/some/nornickel_2024/train_dataset/baseline/da\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mmodule 'numpy' has no attribute 'bool'.\n",
      "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/dm/.local/lib/python3.8/site-packages/skimage/morphology/_skeletonize.py:241: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  0, 1, 1, 0, 0, 1, 0, 0, 0], dtype=np.bool)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/dm/Документы/some/nornickel_2024/train_dataset/baseline/data\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/segment/train5/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.0003' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 90 weight(decay=0.0), 101 weight(decay=0.0005), 100 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/segment/train5\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/100       2.9G     0.6408      1.311     0.4842      1.048          3   \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        565       1312      0.949      0.882      0.927      0.741      0.958      0.872      0.916      0.658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      2/100      2.99G     0.6758      1.383     0.4987      1.068          2   \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        565       1312      0.939      0.882       0.91      0.733      0.938      0.883      0.906      0.656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      3/100       2.9G      0.735      1.472     0.5375      1.088          6   \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        565       1312      0.953      0.866      0.919      0.694      0.952      0.865      0.909      0.631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      4/100       2.9G     0.7418      1.488     0.5605      1.105          3   \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        565       1312      0.928      0.873      0.911      0.719      0.928      0.873        0.9      0.648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      5/100      2.95G     0.7418      1.491      0.542      1.097          6   \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        565       1312      0.951      0.877      0.924      0.721      0.951      0.877       0.92      0.674\n",
      "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 4 epochs. Best results observed at epoch 1, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=4) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.088 hours.\n",
      "Optimizer stripped from runs/segment/train5/weights/last.pt, 6.0MB\n",
      "Optimizer stripped from runs/segment/train5/weights/best.pt, 6.0MB\n",
      "\n",
      "Validating runs/segment/train5/weights/best.pt...\n",
      "Ultralytics 8.3.44 🚀 Python-3.8.19 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce GTX 1660 SUPER, 5929MiB)\n",
      "YOLO11n-seg summary (fused): 265 layers, 2,834,763 parameters, 0 gradients, 10.2 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        565       1312      0.949      0.885      0.927       0.74      0.958      0.872      0.916      0.658\n",
      "Speed: 0.3ms preprocess, 3.7ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/train5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_results = model.train(\n",
    "    data=\"./data.yaml\",  # path to dataset YAML\n",
    "    epochs=100,  # number of training epochs\n",
    "    batch=8,\n",
    "    lr0=3e-4,\n",
    "    patience=4,\n",
    "    imgsz=640,  # training image size\n",
    "    device=0,  # device to run on, i.e. device=0 or device=0,1,2,3 or device=cpu\n",
    "    # evolve=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2660d21c659b40fb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.44 🚀 Python-3.8.19 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce GTX 1660 SUPER, 5929MiB)\n",
      "YOLO11n-seg summary (fused): 265 layers, 2,834,763 parameters, 0 gradients, 10.2 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/dm/Документы/some/nornickel_2024/train_dataset/baseline/data\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        565       1312      0.949      0.883      0.927       0.74      0.957      0.872      0.916      0.658\n",
      "Speed: 0.2ms preprocess, 4.2ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/train52\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = model.val(data=\"./data.yaml\")\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59855c0397ebdf83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T20:45:56.619711200Z",
     "start_time": "2024-12-05T20:45:56.612160500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def infer_image(image_path):\n",
    "    # Загрузка изображения\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Инференс\n",
    "    return model(image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bafd993999b6432d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T20:51:52.367492200Z",
     "start_time": "2024-12-05T20:51:52.357938500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Функция для создания маски с черным фоном\n",
    "def create_mask(image_path, results):\n",
    "    # Загружаем изображение и переводим в градации серого\n",
    "    image = cv2.imread(image_path)\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Создаем пустую маску с черным фоном\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    # Проходим по результатам и создаем маску\n",
    "    for result in results:\n",
    "        masks = result.masks  # Получаем маски из результатов\n",
    "        if masks is not None:\n",
    "            for mask_array in masks.data:  # Получаем маски как массивы\n",
    "                mask_i = mask_array.detach().cpu().numpy()  # Преобразуем маску в numpy массив\n",
    "                \n",
    "                # Изменяем размер маски под размер оригинального изображения\n",
    "                mask_i_resized = cv2.resize(mask_i, (width, height), interpolation=cv2.INTER_LINEAR)\n",
    "                \n",
    "                # Накладываем маску на пустую маску (255 для белого)\n",
    "                mask[mask_i_resized > 0] = 255\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb1b125643cab2da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T20:52:49.360392200Z",
     "start_time": "2024-12-05T20:52:49.205486700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 5 contaminateds, 23.3ms\n",
      "Speed: 3.9ms preprocess, 23.3ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = infer_image(\"./cv_test_dataset/test_img/_07.jpg\")\n",
    "mask_image = create_mask(\"./cv_test_dataset/test_img/_07.jpg\", results)\n",
    "\n",
    "# Сохраняем маску в формате PNG\n",
    "mask_output_path = './mask_image.png'  # Укажите путь для сохранения маски\n",
    "cv2.imwrite(mask_output_path, mask_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bf1867b33cd589",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7e8ad5-b1a5-4dbb-bae2-88e1fe1586a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3fafb2c-c410-4ee4-ac00-9a80502c07d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"baseline6.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a6fad41-0a6b-4227-832f-74464730fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = YOLO(\"baseline1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f04ee15-9f04-44cd-8927-d39f490a7a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7021f1c0-6e01-4108-a86f-e4266ecf4cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
